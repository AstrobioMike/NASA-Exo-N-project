############################################################################################
## Snakefile for Nitrogen Exobio metatranscriptomics mapping to metagenomics assemblies   ##
## Developed by Michael D. Lee (Mike.Lee@nasa.gov)                                        ##
############################################################################################

import os
import pandas as pd

configfile: "config.yaml"


########################################
############# General Info #############
########################################

"""
See the corresponding 'config.yaml' file for general use information. 
Variables that may need to be adjusted should be changed there, not here.
"""


###############################################################
###### Reading info file and parsing into needed objects ######
###############################################################

sample_and_group_info = pd.read_csv(config["sample_info_file"], sep="\t", names = ["sample", "group"])

sample_ID_list = pd.unique(sample_and_group_info["sample"]).tolist()

group_ID_list = pd.unique(sample_and_group_info["group"]).tolist()


################################################
######## Setting up directory structure ########
################################################

dirs_to_create = [config["fastqc_out_dir"], config["filtered_reads_dir"], 
                 config["mapping_dir"], config["combined_output_dir"], config["logs_dir"]]

for dir in dirs_to_create:
    try:
        os.mkdir(dir)
    except:
        pass


###################################################################################
#### Helper functions and setting up variables to handle co-assembly of groups ####
###################################################################################

def group_R1_reads(target_group, sample_and_group_info = sample_and_group_info):
    target_samples_list = list(sample_and_group_info[sample_and_group_info.group.eq(target_group)]['sample'].values)
    return(expand(config["filtered_reads_dir"] + "{ID}" + config["trimmed_R1_suffix"], ID = target_samples_list))

def group_R2_reads(target_group, sample_and_group_info = sample_and_group_info):
    target_samples_list = list(sample_and_group_info[sample_and_group_info.group.eq(target_group)]['sample'].values)
    return(expand(config["filtered_reads_dir"] + "{ID}" + config["trimmed_R2_suffix"], ID = target_samples_list))

R1_group_dict, R2_group_dict = {}, {}
for group in group_ID_list:
    R1_group_dict[group] = group_R1_reads(group)
    R2_group_dict[group] = group_R2_reads(group)


group_sample_bams_list, group_bam_dict = [], {}
def group_sample_bams(target_group, sample_and_group_info = sample_and_group_info):
    target_samples_list = list(sample_and_group_info[sample_and_group_info.group.eq(target_group)]['sample'].values)
    return(expand(config["mapping_dir"] + "{ID}-" + group + ".bam", ID = target_samples_list))

for group in group_ID_list:
    group_sample_bams_list += group_sample_bams(group)
    group_bam_dict[group] = group_sample_bams(group)


group_sample_coverage_files_list, group_cov_dict = [], {}
def group_sample_coverages_files(target_group, sample_and_group_info = sample_and_group_info):
    target_samples_list = list(sample_and_group_info[sample_and_group_info.group.eq(target_group)]['sample'].values)
    return(expand(config["mapping_dir"] + "{ID}-" + group + "-gene-coverages.tsv", ID = target_samples_list))

for group in group_ID_list:
    group_sample_coverage_files_list += group_sample_coverages_files(group)
    group_cov_dict[group] = group_sample_coverages_files(group)


########################################
############# Rules start ##############
########################################

rule all:
    input:
        config["combined_output_dir"] + "All-combined-KO-function-coverages.tsv",
        config["fastqc_out_dir"] + "raw_multiqc_data.zip",
        config["fastqc_out_dir"] + "filtered_multiqc_data.zip"


rule combine_all_gene_coverages_collapsed_by_KO_and_tax:
    conda:
        "envs/bit.yaml"
    input:
        expand(config["combined_output_dir"] + "{group}-gene-coverages-annotations-and-tax.tsv", group = group_ID_list)
    output:
        config["combined_output_dir"] + "All-combined-KO-function-coverages.tsv",
        config["combined_output_dir"] + "All-combined-KO-function-CPM-normalized-coverages.tsv",
        config["combined_output_dir"] + "All-combined-taxonomy-coverages.tsv",
        config["combined_output_dir"] + "All-combined-taxonomy-CPM-normalized-coverages.tsv"
    params:
        combined_output_dir = config["combined_output_dir"]
    shell:
        """
        python scripts/combine-all-gene-tables.py -o {params.combined_output_dir} {input}
        """


rule combine_group_gene_coverage_annots_and_tax:
    conda:
        "envs/bit.yaml"
    input:
        coverage_files = lambda wildcards: group_cov_dict[wildcards.group],
        ko_tab = config["annotations_and_tax_dir"] + "{group}-annotations.tsv",
        tax_tab = config["annotations_and_tax_dir"] + "{group}-gene-tax.tsv"
    output:
        config["combined_output_dir"] + "{group}-gene-coverages-annotations-and-tax.tsv",
        config["combined_output_dir"] + "{group}-CPM-normalized-gene-coverages-annotations-and-tax.tsv"
    params:
        combined_output_dir = config["combined_output_dir"]
    shell:
        """
        python scripts/combine-gene-level-coverages-annots-and-tax-per-group.py -g {wildcards.group} -a {input.ko_tab} -t {input.tax_tab} -o {params.combined_output_dir} {input.coverage_files}
        """


rule get_cov_and_det:
    """
    This rule pulls out coverage and detection information for each sample, gene-level and contig-level,
    and filters the coverage information based on requiring at least 50% detection.
    """

    conda:
        "envs/mapping.yaml"
    input:
        bam = config["mapping_dir"] + "{ID}-{group}.bam",
        nt = config["genes_dir"] + "{group}-genes.fa"
    params:
        gene_cov_and_det_tmp = config["mapping_dir"] + "{ID}-{group}-gene-cov-and-det.tmp",
        contig_cov_and_det_tmp = config["mapping_dir"] + "{ID}-{group}-contig-cov-and-det.tmp",
        gene_cov_tmp = config["mapping_dir"] + "{ID}-{group}-gene-cov.tmp",
        contig_cov_tmp = config["mapping_dir"] + "{ID}-{group}-contig-cov.tmp"
    output:
        gene_covs = config["mapping_dir"] + "{ID}-{group}-gene-coverages.tsv",
        contig_covs = config["mapping_dir"] + "{ID}-{group}-contig-coverages.tsv"
    log:
        config["logs_dir"] + "{ID}-{group}-pileup.log"
    shell:
        """
        pileup.sh -in {input.bam} fastaorf={input.nt} outorf={params.gene_cov_and_det_tmp} out={params.contig_cov_and_det_tmp} > {log} 2>&1

        # filtering coverages based on detection
          # genes
        grep -v "#" {params.gene_cov_and_det_tmp} | awk -F $'\t' ' BEGIN {{OFS=FS}} {{ if ( $10 <= 0.5 ) $4 = 0 }} {{ print $1,$4 }} ' > {params.gene_cov_tmp}
        cat <( printf "gene_ID\tcoverage\n" ) {params.gene_cov_tmp} > {output.gene_covs}

          # contig
        grep -v "#" {params.contig_cov_and_det_tmp} | awk -F $'\t' ' BEGIN {{OFS=FS}} {{ if ( $5 <= 50 ) $2 = 0 }} {{ print $1,$2 }} ' > {params.contig_cov_tmp}
        cat <( printf "contig_ID\tcoverage\n" ) {params.contig_cov_tmp} > {output.contig_covs}

          # removing intermediate files
        rm {params}
        """


rule run_mapping:
    """
    This rule runs the mapping for each sample to its appropriate group coassembly.
    """
    
    conda:
        "envs/mapping.yaml"
    input:
        R1 = config["filtered_reads_dir"] + "{ID}" + config["trimmed_R1_suffix"],
        R2 = config["filtered_reads_dir"] + "{ID}" + config["trimmed_R2_suffix"]
    params:
        index = config["bowtie2_indices_dir"] + "{group}-index",
        mapping_info = config["logs_dir"] + "{ID}-{group}-mapping-info.txt",
        num_threads = config["num_threads"]
    output:
        config["mapping_dir"] + "{ID}-{group}.bam"
    shell:
        """
        bowtie2 --mm -q --threads {params.num_threads} -x {params.index} -1 {input.R1} -2 {input.R2} --no-unal 2> {params.mapping_info} | samtools view -b | samtools sort -@ {params.num_threads} > {output} 2> /dev/null
        samtools index -@ {params.num_threads} {output}
        """


rule filtered_fastqc:
    """
    This rule runs fastqc on all trimmed/filtered input fastq files.
    """

    conda:
        "envs/qc.yaml"
    input:
        config["filtered_reads_dir"] + "{ID}" + config["trimmed_R1_suffix"],
        config["filtered_reads_dir"] + "{ID}" + config["trimmed_R2_suffix"]
    output:
        config["filtered_reads_dir"] + "{ID}" + config["trimmed_R1_fastqc_suffix"],
        config["filtered_reads_dir"] + "{ID}" + config["trimmed_R2_fastqc_suffix"]
    shell:
        """
		fastqc {input} -t 2 -q
		"""


rule filtered_multiqc:
    """
    This rule collates all trimmed/filtered fastqc outputs.
    """

    conda:
        "envs/qc.yaml"
    input:
        expand(config["filtered_reads_dir"] + "{ID}" + config["trimmed_R1_fastqc_suffix"], ID = sample_ID_list),
        expand(config["filtered_reads_dir"] + "{ID}" + config["trimmed_R2_fastqc_suffix"], ID = sample_ID_list)
    params:
        fastqc_out_dir = config["fastqc_out_dir"],
        filtered_reads_dir = config["filtered_reads_dir"]
    output:
        config["fastqc_out_dir"] + "filtered_multiqc.html",
        config["fastqc_out_dir"] + "filtered_multiqc_data.zip"
    shell:
        """
        multiqc -z -q -o {params.fastqc_out_dir} -n filtered_multiqc  {params.filtered_reads_dir} > /dev/null 2>&1
          # removing the individual fastqc files and temp locations
        rm -rf {params.filtered_reads_dir}*fastqc*
        """


rule bbduk:
    """
    This rule runs quality filtering/trimming on raw input fastq files for each individual sample.
    """

    conda:
        "envs/qc.yaml"
    input:
        in1 = config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"],
        in2 = config["raw_reads_dir"] + "{ID}" + config["raw_R2_suffix"]
    output:
        out1 = config["filtered_reads_dir"] + "{ID}" + config["trimmed_R1_suffix"],
        out2 = config["filtered_reads_dir"] + "{ID}" + config["trimmed_R2_suffix"]
    log:
        config["logs_dir"] + "bbduk-{ID}.log"
    shell:
        """
        bbduk.sh in={input.in1} in2={input.in2} out1={output.out1} out2={output.out2} \
                ref=${{CONDA_PREFIX}}/opt/bbmap-38.86-0/resources/adapters.fa ktrim=l k=17 ftm=5 qtrim=rl \
                trimq=10 mlf=0.5 maxns=0 > {log} 2>&1
        """


rule raw_multiqc:
    """
    This rule collates all raw fastqc outputs.
    """

    conda:
        "envs/qc.yaml"
    input:
        expand(config["raw_reads_dir"] + "{ID}" + config["raw_R1_fastqc_suffix"], ID = sample_ID_list),
        expand(config["raw_reads_dir"] + "{ID}" + config["raw_R2_fastqc_suffix"], ID = sample_ID_list)
    params:
        raw_reads_dir = config["raw_reads_dir"],
        fastqc_out_dir = config["fastqc_out_dir"]
    output:
        config["fastqc_out_dir"] + "raw_multiqc.html",
        config["fastqc_out_dir"] + "raw_multiqc_data.zip"
    shell:
        """
        multiqc -z -q -o {params.fastqc_out_dir} -n raw_multiqc {params.raw_reads_dir} > /dev/null 2>&1
          # removing the individual fastqc files
        rm -rf {params.raw_reads_dir}*fastqc*
        """


rule raw_fastqc:
    """
    This rule runs fastqc on all raw input fastq files.
    """

    conda:
        "envs/qc.yaml"
    input:
        config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"],
        config["raw_reads_dir"] + "{ID}" + config["raw_R2_suffix"]        
    output:
        config["raw_reads_dir"] + "{ID}" + config["raw_R1_fastqc_suffix"],
        config["raw_reads_dir"] + "{ID}" + config["raw_R2_fastqc_suffix"]
    shell:
        """
		fastqc {input} -t 2 -q
		"""



rule clean_all:
    shell:
        "rm -rf {dirs_to_create} .snakemake/ snakemake-run.log"
