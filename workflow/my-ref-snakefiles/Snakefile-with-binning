import os


########################################
############# General Info #############
########################################
"""
This is expected to be run in the following conda environment (or with specified programs versions accessible):

# conda create -y -n BRAILLE-assembly -c conda-forge -c bioconda -c defaults -c astrobiomike -c ursky \
#              fastqc=0.11.9 multiqc=1.8 bbmap=38.79 megahit=1.2.9 bit=1.8.10 bowtie2=2.3.5.1 \
#              samtools=1.9 prodigal=2.6.3 kofamscan=1.3.0 blast=2.9.0 cat=5.1.2 metabat2=2.15 \
#              checkm-genome=1.1.2 gtdbtk=1.3.0 snakemake=5.5.4=1

# conda activate BRAILLE-assembly

The KOFamScan reference files can be downloaded as follows:

# curl -LO ftp://ftp.genome.jp/pub/db/kofam/profiles.tar.gz
# curl -LO ftp://ftp.genome.jp/pub/db/kofam/ko_list.gz
# tar -xzvf profiles.tar.gz
# gunzip ko_list.gz
As written, the directory holding them should be in this "KO_DIR" shell env. variable
# KO_DIR=$(pwd)
    # this would set it if run in the directory they were just downloaded and unpacked in
    # it can be permanently added to the conda environment like so (modified to be the path to the directory, but this location will work if the conda environment was named the same way as above):
# echo 'export KO_DIR=/path/to/kofamscan_db' >> ${CONDA_PREFIX}/etc/conda/activate.d/set_env_vars.sh
    # then deactivate and reactivate conda environment (same goes for CAT_DIR next)

The CAT reference files can be downloaded as follows:
# wget tbb.bio.uu.nl/bastiaan/CAT_prepare/CAT_prepare_20200618.tar.gz
# tar -xvzf CAT_prepare_20200618.tar.gz
# As written, the directory holding them should be in this "CAT_DIR" shell env. variable
# CAT_DIR=$(pwd)/CAT_prepare_20200618
    # this would set it if ran in the directory they were just downloaded and unpacked in
    # it can be permanentily added to the conda environment like noted above for the KO_DIR variable:
# echo 'export CAT_DIR=/path/to/CAT_prepare_20200618' >> ${CONDA_PREFIX}/etc/conda/activate.d/set_env_vars.sh
    # then deactivate and reactivate conda environment

The checkm reference directory needs to be downloaded and set, was done with the following
# execute in the directory where it wants to be stored
# curl -LO https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gz
# tar -xzvf checkm_data_2015_01_16.tar.gz
# checkm data setRoot $PWD

The gtdbtk reference directory needs to be downloaded and set, can be done with their helper script in the conda environment
# download-db.sh
"""


########################################
######## Setting some variables ########
########################################
  # ran VAL and YEL separately, just need to change what's commented here
# current_cave = "VAL"
current_cave = "YEL"

  # single column file holding unique portion of sample names
sample_IDs_file = current_cave + "-samples.txt"

  # any helpful prefixes or suffixes
trimmed_filename_R1_suffix = "_R1_trimmed.fq.gz"
trimmed_filename_R2_suffix = "_R2_trimmed.fq.gz"

filename_R1_suffix = "_R1_raw.fastq.gz"
filename_R2_suffix = "_R2_raw.fastq.gz"

raw_fastqc_R1_suffix = "_R1_raw_fastqc.zip"
raw_fastqc_R2_suffix = "_R2_raw_fastqc.zip"

trimmed_fastqc_R1_suffix = "_R1_trimmed_fastqc.zip"
trimmed_fastqc_R2_suffix = "_R2_trimmed_fastqc.zip"

  # directories (all relative to processing directory)
raw_reads_dir = "../../raw-data/"
fastqc_out_dir = "../fastqc-outputs/"
filtered_reads_dir = "../../trimmed-data/"
assemblies_dir = "../assemblies/"
genes_dir = "../predicted-genes/"
annotations_and_tax_dir = "../annotations-and-taxonomy/"
mapping_dir = "../read-mapping/"
combined_output_dir = "../combined-outputs/"
bins_dir = "../bins/"
MAGs_dir = "../MAGs/"

needed_dirs_to_create = [fastqc_out_dir, filtered_reads_dir, assemblies_dir, genes_dir, annotations_and_tax_dir, mapping_dir, combined_output_dir, bins_dir, MAGs_dir]

  # number of threads to use PER snakemake job started (that's determined by the -j parameter passed to the snakemake call)
    # passed to fastqc, bowtie2, samtools, metabat2, checkm-pplacer
num_threads = 5

  # number of cpus to use PER snakemake job started
    # passed to KOFamScan, CAT, checkm
num_cpus = 5

  # number of cpus to use during gtdb-tk (only one run when all MAGs are done)
gtdbtk_cpus = 25

  # getting KOFamScan ref directory location (see General Info above)
KO_DIR = os.environ.get("KO_DIR")

  # getting CAT ref directory location (see General Info above)
CAT_DIR = os.environ.get("CAT_DIR")


########################################
#### Reading samples file into list ####
########################################

sample_ID_list = [line.strip() for line in open(sample_IDs_file)]


########################################
######## Setting up directories ########
########################################

for dir in needed_dirs_to_create:
	try:
		os.mkdir(dir)
	except:
		pass


for dir in expand(bins_dir + "{ID}", ID = sample_ID_list):
    try:
        os.mkdir(dir)
    except:
        pass

########################################
############# Rules start ##############
########################################


rule all:
    input:
        expand(mapping_dir + "{ID}-metabat-assembly-depth.tsv", ID = sample_ID_list),
        expand(MAGs_dir + "{ID}-MAGs-checkm-out.tsv", ID = sample_ID_list),
        MAGs_dir + current_cave + "-gtdbtk-out/",
        MAGs_dir + current_cave + "-MAGs-assembly-summaries.tsv",
        MAGs_dir + current_cave + "-MAGs-overview.tsv",
        combined_output_dir + current_cave + "-KO-function-coverages.tsv",
        combined_output_dir + current_cave + "-taxonomy-coverages.tsv",
        expand(annotations_and_tax_dir + "{ID}-contig-coverage-and-tax.tsv", ID = sample_ID_list),
        expand(annotations_and_tax_dir + "{ID}-annotations.tsv", ID = sample_ID_list),
        assemblies_dir + "assembly-summaries.tsv",
        fastqc_out_dir + "raw_multiqc_report.html",
        fastqc_out_dir + "raw_multiqc_data.zip",
        fastqc_out_dir + "trimmed_multiqc_report.html",
        fastqc_out_dir + "trimmed_multiqc_data.zip"


rule run_metabat:
    input:
        assembly = assemblies_dir + "{ID}-assembly.fa",
        bam = mapping_dir + "{ID}.bam"
    params:
        prefix = bins_dir + "{ID}"
    output:
        depth_file = mapping_dir + "{ID}-metabat-assembly-depth.tsv",
    shell:
        """
        jgi_summarize_bam_contig_depths --outputDepth {output.depth_file} --percentIdentity 97 --minContigLength 1000 --minContigDepth 1.0  --referenceFasta {input.assembly} {input.bam} > /dev/null 2>&1
        metabat2  --inFile {input.assembly} --outFile {params.prefix}/bin --abdFile {output.depth_file} -t {num_threads} > /dev/null 2>&1
        """


rule run_checkm_on_bins:
    input:
        curr_bins_dir = bins_dir + "{ID}/",
        trigger = mapping_dir + "{ID}-metabat-assembly-depth.tsv"
    params:
        tmp_dir = bins_dir + "{ID}/checkm-out-tmp/"
    output:
        bins_dir + "{ID}-metabat-checkm-out.tsv"
    shell:
        """
        checkm lineage_wf -f {output} --tab_table -t {num_cpus} --pplacer_threads {num_threads} -x fa {input.curr_bins_dir} {params.tmp_dir} > /dev/null 2>&1
        rm -rf {params.tmp_dir}
        """


rule filtering_checkm_results_and_copying_MAGs:
    input:
        bins_dir + "{ID}-metabat-checkm-out.tsv"
    output:
        MAGs_dir + "{ID}-MAGs-checkm-out.tsv"
    shell:
        """
        mkdir {MAGs_dir}{wildcards.ID}/

        cat <( head -n 1 {input} ) <( awk -F $'\\t' ' $12 >= 90 && $13 <= 10 && $14 == 0 ' {input} ) > {wildcards.ID}-MAG-info.tmp

        sed 's/bin./{wildcards.ID}-MAG-/' {wildcards.ID}-MAG-info.tmp > {output}

        for MAG in $(cut -f 1 {wildcards.ID}-MAG-info.tmp | tail -n +2)
        do
            new_ID=$(echo $MAG | sed 's/bin./MAG-/')
            cp {bins_dir}{wildcards.ID}/${{MAG}}.fa {MAGs_dir}{wildcards.ID}/{wildcards.ID}-${{new_ID}}.fa
        done

        rm {wildcards.ID}-MAG-info.tmp
        """


rule run_gtdbtk_on_MAGs:
    input:
        trigger = expand(bins_dir + "{ID}-metabat-checkm-out.tsv", ID = sample_ID_list)
    params:
        temp_all_MAGs_dir = MAGs_dir + current_cave + "-all-for-gtdb/"
    output:
        directory(MAGs_dir + current_cave + "-gtdbtk-out/")
    log:
        MAGs_dir + current_cave + "-gtdbtk-out/gtdbtk-run.log"
    shell:
        """
        mkdir -p {params.temp_all_MAGs_dir}

        cp {MAGs_dir}{current_cave}*/*.fa {params.temp_all_MAGs_dir}

        gtdbtk classify_wf --genome_dir {params.temp_all_MAGs_dir} -x fa --out_dir {output} --cpus {gtdbtk_cpus} > {log} 2>&1

        rm -rf {params.temp_all_MAGs_dir}
        """


rule summarize_MAG_assemblies:
    input:
        trigger = expand(bins_dir + "{ID}-metabat-checkm-out.tsv", ID = sample_ID_list)
    params:
        intermediate_file = MAGs_dir + current_cave + "-MAGs-summaries.tmp"
    output:
        MAGs_dir + current_cave + "-MAGs-assembly-summaries.tsv"
    shell:
        """
        bit-summarize-assembly {MAGs_dir}{current_cave}*/*.fa -o {params.intermediate_file} -t

        # slimming down the output
        cut -f 1,2,3,5,6,8,11,18,19,20 {params.intermediate_file} > {output}
        rm {params.intermediate_file}
        """


rule generate_MAGs_overview_table:
    input:
        assembly_summaries = MAGs_dir + current_cave + "-MAGs-assembly-summaries.tsv",
        checkm_results = expand(MAGs_dir + "{ID}-MAGs-checkm-out.tsv", ID = sample_ID_list),
        gtdb_trigger = MAGs_dir + current_cave + "-gtdbtk-out"
    params:
        gtdb_results = MAGs_dir + current_cave + "-gtdbtk-out/gtdbtk.*.summary.tsv",
        checkm_tmp = MAGs_dir + current_cave + "-checkm-estimates.tmp",
        gtdb_tmp = MAGs_dir + current_cave + "-gtdb-taxonomies.tmp",
        checkm_w_header_tmp = MAGs_dir + current_cave + "-checkm-estimates-with-headers.tmp",
        gtdb_w_header_tmp = MAGs_dir + current_cave + "-gtdb-taxonomies-with-headers.tmp",
        overview_tmp = MAGs_dir + current_cave + "-MAGs-overview.tmp",
        overview_header_tmp = MAGs_dir + current_cave + "-MAGs-overview-header.tmp",
        overview_sorted_tmp = MAGs_dir + current_cave + "-MAGs-overview-sorted.tmp"
    output:
        MAGs_dir + current_cave + "-MAGs-overview.tsv"
    shell:
        """
        # making sure none of the intermediate files exist already
        rm -rf {params.checkm_tmp} {params.gtdb_tmp} {params.checkm_w_header_tmp} {params.gtdb_w_header_tmp} {params.overview_tmp} {params.overview_header_tmp} {params.overview_sorted_tmp}

        for MAG in $(cut -f 1 {input.assembly_summaries} | tail -n +2)
        do

            grep -w "^${{MAG}}" {input.checkm_results} | cut -f 12,13 >> {params.checkm_tmp}

            # note: below won't work with darwin sed due to tab character
            grep -w "^${{MAG}}" {params.gtdb_results} | cut -f 2 | sed 's/^.__//' | sed 's/;.__/\\t/g' | awk -F $'\\t' ' BEGIN {{ OFS=FS }} {{ for (i=1; i<=NF; i++) if ( $i ~/^ *$/) $i = "NA" }}; 1 ' >> {params.gtdb_tmp}

        done

        # adding headers
        cat <(printf "est. completeness\\test. redundancy\\n") {params.checkm_tmp} > {params.checkm_w_header_tmp}
        cat <(printf "domain\\tphylum\\tclass\\torder\\tfamily\\tgenus\\tspecies\\n") {params.gtdb_tmp} > {params.gtdb_w_header_tmp}

        paste {input.assembly_summaries} {params.checkm_w_header_tmp} {params.gtdb_w_header_tmp} > {params.overview_tmp}

        # ordering by taxonomy
        head -n 1 {params.overview_tmp} > {params.overview_header_tmp}
        tail -n +2 {params.overview_tmp} | sort -t $'\\t' -k 13,19 > {params.overview_sorted_tmp}

        cat {params.overview_header_tmp} {params.overview_sorted_tmp} > {output}

        rm -rf {params.checkm_tmp} {params.gtdb_tmp} {params.checkm_w_header_tmp} {params.gtdb_w_header_tmp} {params.overview_tmp} {params.overview_header_tmp} {params.overview_sorted_tmp}
        """


rule make_combined_tables:
    input:
        expand(annotations_and_tax_dir + "{ID}-gene-coverage-annotation-and-tax.tsv", ID = sample_ID_list)
    params:
        out_prefix = combined_output_dir + current_cave
    output:
        combined_annots = combined_output_dir + current_cave + "-KO-function-coverages.tsv",
        combined_tax = combined_output_dir + current_cave + "-taxonomy-coverages.tsv"
    shell:
        """
        bit-GL-combine-KO-and-tax-tables {input} -o {params.out_prefix}
          # shortening column names to unique part of sample names
        sed -i.tmp 's/-gene-coverage-annotation-and-tax//g' {output.combined_annots} && rm {output.combined_annots}.tmp
        sed -i.tmp 's/-gene-coverage-annotation-and-tax//g' {output.combined_tax} && rm {output.combined_tax}.tmp
        """


rule combine_contig_tax_and_coverage:
    input:
        cov = mapping_dir + "{ID}-contig-coverages.tsv",
        tax = annotations_and_tax_dir + "{ID}-contig-tax.tsv"
    params:
        contig_tmp = annotations_and_tax_dir + "{ID}-contig.tmp",
        header_tmp = annotations_and_tax_dir + "{ID}-contig-header.tmp"
    output:
        annotations_and_tax_dir + "{ID}-contig-coverage-and-tax.tsv"
    shell:
        """
        paste <( tail -n +2 {input.cov} | sort -V -k 1 ) <( tail -n +2 {input.tax} | sort -V -k 1 | cut -f 2- ) > {params.contig_tmp}
        paste <( head -n 1 {input.cov} ) <( head -n 1 {input.tax} | cut -f 2- ) > {params.header_tmp}
        cat {params.header_tmp} {params.contig_tmp} > {output}
        rm {params}
        """


rule combine_gene_annots_tax_and_coverage:
    input:
        cov = mapping_dir + "{ID}-gene-coverages.tsv",
        annots = annotations_and_tax_dir + "{ID}-annotations.tsv",
        tax = annotations_and_tax_dir + "{ID}-gene-tax.tsv"
    params:
        gene_tmp = annotations_and_tax_dir + "{ID}-gene.tmp",
        header_tmp = annotations_and_tax_dir + "{ID}-gene-header.tmp"
    output:
        annotations_and_tax_dir + "{ID}-gene-coverage-annotation-and-tax.tsv"
    shell:
        """
        paste <( tail -n +2 {input.cov} | sort -V -k 1 ) <( tail -n +2 {input.annots} | sort -V -k 1 | cut -f 2- ) <( tail -n +2 {input.tax} | sort -V -k 1 | cut -f 2- ) > {params.gene_tmp}
        paste <( head -n 1 {input.cov} ) <( head -n 1 {input.annots} | cut -f 2- ) <( head -n 1 {input.tax} | cut -f 2- ) > {params.header_tmp}

        cat {params.header_tmp} {params.gene_tmp} > {output}

        rm {params}
        """


rule get_cov_and_det:
    input:
        bam = mapping_dir + "{ID}.bam",
        nt = genes_dir + "{ID}-genes.fa"
    params:
        gene_cov_and_det_tmp = mapping_dir + "{ID}-gene-cov-and-det.tmp",
        contig_cov_and_det_tmp = mapping_dir + "{ID}-contig-cov-and-det.tmp",
        gene_cov_tmp = mapping_dir + "{ID}-gene-cov.tmp",
        contig_cov_tmp = mapping_dir + "{ID}-contig-cov.tmp"
    output:
        gene_covs = mapping_dir + "{ID}-gene-coverages.tsv",
        contig_covs = mapping_dir + "{ID}-contig-coverages.tsv"
    shell:
        """
        pileup.sh -in {input.bam} fastaorf={input.nt} outorf={params.gene_cov_and_det_tmp} out={params.contig_cov_and_det_tmp} > /dev/null 2>&1

        # filtering coverages based on detection
          # genes
        grep -v "#" {params.gene_cov_and_det_tmp} | awk -F $'\t' ' BEGIN {{OFS=FS}} {{ if ( $10 <= 0.5 ) $4 = 0 }} {{ print $1,$4 }} ' > {params.gene_cov_tmp}
        cat <( printf "gene_ID\tcoverage\n" ) {params.gene_cov_tmp} > {output.gene_covs}

          # contigs
        grep -v "#" {params.contig_cov_and_det_tmp} | awk -F $'\t' ' BEGIN {{OFS=FS}} {{ if ( $5 <= 50 ) $2 = 0 }} {{ print $1,$2 }} ' > {params.contig_cov_tmp}
        cat <( printf "contig_ID\tcoverage\n" ) {params.contig_cov_tmp} > {output.contig_covs}

          # removing intermediate files
        rm {params}
        """


rule run_mapping:
    input:
        assembly = assemblies_dir + "{ID}-assembly.fa",
        R1 = filtered_reads_dir + "{ID}" + trimmed_filename_R1_suffix,
        R2 = filtered_reads_dir + "{ID}" + trimmed_filename_R2_suffix
    params:
        index = mapping_dir + "{ID}-index"
    output:
        mapping_dir + "{ID}.bam"
    log:
        mapping_dir + "{ID}-mapping.log"
    shell:
        """
        bowtie2-build {input.assembly} {params.index} > /dev/null 2>&1
        bowtie2 --threads {num_threads} -x {params.index} -1 {input.R1} -2 {input.R2} 2> {log} | samtools view -b | samtools sort -@ {num_threads} > {output} 2> /dev/null
        samtools index -@ {num_threads} {output}

        # rm {params.index}*
        """


rule run_tax_classification:
    input:
        assembly = assemblies_dir + "{ID}-assembly.fa",
        AA = genes_dir + "{ID}-genes.faa"
    output:
        gene_tax_out = annotations_and_tax_dir + "{ID}-gene-tax.tsv",
        contig_tax_out = annotations_and_tax_dir + "{ID}-contig-tax.tsv"
    params:
        CAT_DB = CAT_DIR + "/2020-06-18_database/",
        CAT_TAX = CAT_DIR + "/2020-06-18_taxonomy/",
        tmp_out_prefix = annotations_and_tax_dir + "{ID}-tax-out.tmp",
        tmp_genes = annotations_and_tax_dir + "{ID}-gene-tax.tmp",
        tmp_contigs = annotations_and_tax_dir + "{ID}-contig-tax.tmp"
    shell:
        """
        CAT contigs -d {params.CAT_DB} -t {params.CAT_TAX} -n {num_cpus} -r 3 --top 4 --I_know_what_Im_doing -c {input.assembly} -p {input.AA} -o {params.tmp_out_prefix} > /dev/null 2>&1

        # adding names to gene classifications
        CAT add_names -i {params.tmp_out_prefix}.ORF2LCA.txt -o {params.tmp_genes} -t {params.CAT_TAX} --only_official > /dev/null 2>&1

        # formatting gene classifications
        awk -F $'\t' ' BEGIN {{ OFS=FS }} {{ if ( $2 == "lineage" ) {{ print $1,$2,$4,$5,$6,$7,$8,$9,$10 }} \
        else if ( $2 == "ORF has no hit to database" || $2 ~ /^no taxid found/ ) {{ print $1,"NA","NA","NA","NA","NA","NA","NA","NA" }} \
        else {{ n=split($2,lineage,";"); print $1,lineage[n],$4,$5,$6,$7,$8,$9,$10 }} }} ' {params.tmp_genes} | \
        sed 's/not classified/NA/g' | sed 's/superkingdom/domain/' | sed 's/^# ORF/gene_ID/' | sed 's/lineage/taxid/' | \
        sed 's/\*//g' > {output.gene_tax_out}

        # adding names to contig classifications
        CAT add_names -i {params.tmp_out_prefix}.contig2classification.txt -o {params.tmp_contigs} -t {params.CAT_TAX} --only_official > /dev/null 2>&1

        # formatting contig classifications
        awk -F $'\t' ' BEGIN {{ OFS=FS }} {{ if ( $2 == "classification" ) {{ print $1,$4,$6,$7,$8,$9,$10,$11,$12 }} \
        else if ( $2 == "unclassified" ) {{ print $1,"NA","NA","NA","NA","NA","NA","NA","NA" }} \
        else {{ n=split($4,lineage,";"); print $1,lineage[n],$6,$7,$8,$9,$10,$11,$12 }} }} ' {params.tmp_contigs} | \
        sed 's/not classified/NA/g' | sed 's/superkingdom/domain/' | sed 's/: [0-9\.]*//g' | sed 's/^# contig/contig_ID/' | \
        sed 's/lineage/taxid/' | sed 's/\*//g' > {output.contig_tax_out}

        # rm -rf ../Functional_Annotations_and_Taxonomy/{wildcards.ID}*.tmp*
        """


rule run_KO_annotation:
    input:
        genes_dir + "{ID}-genes.faa"
    output:
        annotations_and_tax_dir + "{ID}-annotations.tsv"
    params:
        KO_profiles = KO_DIR + "/profiles/",
        KO_list = KO_DIR + "/ko_list",
        tmp_out = annotations_and_tax_dir + "{ID}-KO-tab.tmp",
        tmp_dir = annotations_and_tax_dir + "{ID}-tmp-KO-dir"
    shell:
        """
        exec_annotation -p {params.KO_profiles} -k {params.KO_list} --cpu {num_cpus} -f detail-tsv -o {params.tmp_out} --tmp-dir {params.tmp_dir} --report-unannotated {input} 

        bit-filter-KOFamScan-results -i {params.tmp_out} -o {output}

        # rm -rf {params.tmp_out} {params.tmp_dir} 
        """


rule call_genes:
    input:
        assemblies_dir + "{ID}-assembly.fa"
    output:
        AA = genes_dir + "{ID}-genes.faa",
        nt = genes_dir + "{ID}-genes.fa",
        gff = genes_dir + "{ID}-genes.gff"
    shell:
        """
        prodigal -q -c -p meta -a {output.AA} -d {output.nt} -f gff -o {output.gff} -i {input}
        """


rule summarize_assemblies:
    input:
        expand(assemblies_dir + "{ID}-assembly.fa", ID = sample_ID_list),
    output:
        assemblies_dir + "assembly-summaries.tsv"
    shell:
        """
        bit-summarize-assembly -o {output} {input}
        """


rule assemble:
    input:
        R1 = filtered_reads_dir + "{ID}" + trimmed_filename_R1_suffix,
        R2 = filtered_reads_dir + "{ID}" + trimmed_filename_R2_suffix
    output:
        assemblies_dir + "{ID}-assembly.fa"
    params:
        num_threads
    log:
        assemblies_dir + "{ID}-assembly.log"
    shell:
        """
        megahit -1 {input.R1} -2 {input.R1} -m 500 -t {params} -o {assemblies_dir}{wildcards.ID}-megahit-out > {log} 2>&1
        bit-rename-fasta-headers -i {assemblies_dir}{wildcards.ID}-megahit-out/final.contigs.fa -w c_{wildcards.ID} -o {output}

        rm -rf {assemblies_dir}{wildcards.ID}-megahit-out/
        """


rule raw_fastqc:
    input:
        raw_reads_dir + "{ID}" + filename_R1_suffix,
        raw_reads_dir + "{ID}" + filename_R2_suffix        
    output:
        raw_reads_dir + "{ID}" + raw_fastqc_R1_suffix,
        raw_reads_dir + "{ID}" + raw_fastqc_R2_suffix
    shell:
        """
		fastqc {input} -t {num_threads} -q
		"""


rule raw_multiqc:
    input:
        expand(raw_reads_dir + "{ID}" + raw_fastqc_R1_suffix, ID = sample_ID_list),
        expand(raw_reads_dir + "{ID}" + raw_fastqc_R2_suffix, ID = sample_ID_list)
    output:
        fastqc_out_dir + "raw_multiqc_report.html",
        fastqc_out_dir + "raw_multiqc_data.zip"
    shell:
        """
        multiqc -z -q -o {fastqc_out_dir}raw_tmp_qc {raw_reads_dir} > /dev/null 2>&1
          # renaming the outputs
        mv {fastqc_out_dir}raw_tmp_qc/multiqc_data.zip {fastqc_out_dir}raw_multiqc_data.zip
        mv {fastqc_out_dir}raw_tmp_qc/multiqc_report.html {fastqc_out_dir}raw_multiqc_report.html
        rm -rf {fastqc_out_dir}raw_tmp_qc
          # removing the individual fastqc files
        rm {raw_reads_dir}*fastqc*
        """


rule bbduk:
    input:
        in1 = raw_reads_dir + "{ID}" + filename_R1_suffix,
        in2 = raw_reads_dir + "{ID}" + filename_R2_suffix
    output:
        out1 = filtered_reads_dir + "{ID}" + trimmed_filename_R1_suffix,
        out2 = filtered_reads_dir + "{ID}" + trimmed_filename_R2_suffix
    log:
        filtered_reads_dir + "bbduk-{ID}.log"
    shell:
        """
        bbduk.sh in={input.in1} in2={input.in2} out1={output.out1} out2={output.out2} \
                ref=${{CONDA_PREFIX}}/opt/bbmap-38.86-0/resources/adapters.fa ktrim=l ftm=5 qtrim=rl \
                trimq=15 minlength=150 > {log} 2>&1
        """

rule trimmed_fastqc:
    input:
        filtered_reads_dir + "{ID}" + trimmed_filename_R1_suffix,
        filtered_reads_dir + "{ID}" + trimmed_filename_R2_suffix
    output:
        filtered_reads_dir + "{ID}" + trimmed_fastqc_R1_suffix,
        filtered_reads_dir + "{ID}" + trimmed_fastqc_R2_suffix
    shell:
        """
		fastqc {input} -t {num_threads} -q
		"""


rule trimmed_multiqc:
    input:
        expand(filtered_reads_dir + "{ID}" + trimmed_fastqc_R1_suffix, ID = sample_ID_list),
        expand(filtered_reads_dir + "{ID}" + trimmed_fastqc_R2_suffix, ID = sample_ID_list)
    output:
        fastqc_out_dir + "trimmed_multiqc_report.html",
        fastqc_out_dir + "trimmed_multiqc_data.zip"
    shell:
        """
        multiqc -z -q -o {fastqc_out_dir}trimmed_tmp_qc {filtered_reads_dir} > /dev/null 2>&1
          # renaming the outputs
        mv {fastqc_out_dir}trimmed_tmp_qc/multiqc_data.zip {fastqc_out_dir}trimmed_multiqc_data.zip
        mv {fastqc_out_dir}trimmed_tmp_qc/multiqc_report.html {fastqc_out_dir}trimmed_multiqc_report.html
        rm -rf {fastqc_out_dir}trimmed_tmp_qc
          # removing the individual fastqc files
        rm {filtered_reads_dir}*fastqc*
        """


# rule clean_all:
#     shell:
#         "rm -rf {needed_dirs_to_create}"
