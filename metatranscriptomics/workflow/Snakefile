############################################################################################
## Snakefile for Nitrogen Exobio metagenomics                                             ##
## Developed by Michael D. Lee (Mike.Lee@nasa.gov)                                        ##
############################################################################################

import os
import pandas as pd

configfile: "config.yaml"


########################################
############# General Info #############
########################################

"""
See the corresponding 'config.yaml' file for general use information. 
Variables that may need to be adjusted should be changed there, not here.
"""


###############################################################
###### Reading info file and parsing into needed objects ######
###############################################################

sample_and_group_info = pd.read_csv(config["sample_info_file"], sep="\t", names = ["sample", "group"])

sample_ID_list = pd.unique(sample_and_group_info["sample"]).tolist()

group_ID_list = pd.unique(sample_and_group_info["group"]).tolist()


################################################
######## Setting up directory structure ########
################################################

dirs_to_create = [config["fastqc_out_dir"], config["filtered_reads_dir"], config["assemblies_dir"], 
                 config["genes_dir"], config["annotations_and_tax_dir"], config["mapping_dir"], 
                 config["combined_output_dir"], config["logs_dir"]]

for dir in dirs_to_create:
    try:
        os.mkdir(dir)
    except:
        pass


###################################################################################
#### Helper functions and setting up variables to handle co-assembly of groups ####
###################################################################################

def group_R1_reads(target_group, sample_and_group_info = sample_and_group_info):
    target_samples_list = list(sample_and_group_info[sample_and_group_info.group.eq(target_group)]['sample'].values)
    return(expand(config["filtered_reads_dir"] + "{ID}" + config["trimmed_R1_suffix"], ID = target_samples_list))

def group_R2_reads(target_group, sample_and_group_info = sample_and_group_info):
    target_samples_list = list(sample_and_group_info[sample_and_group_info.group.eq(target_group)]['sample'].values)
    return(expand(config["filtered_reads_dir"] + "{ID}" + config["trimmed_R2_suffix"], ID = target_samples_list))

R1_group_dict, R2_group_dict = {}, {}
for group in group_ID_list:
    R1_group_dict[group] = group_R1_reads(group)
    R2_group_dict[group] = group_R2_reads(group)


group_sample_bams_list, group_bam_dict = [], {}
def group_sample_bams(target_group, sample_and_group_info = sample_and_group_info):
    target_samples_list = list(sample_and_group_info[sample_and_group_info.group.eq(target_group)]['sample'].values)
    return(expand(config["mapping_dir"] + "{ID}-" + group + ".bam", ID = target_samples_list))

for group in group_ID_list:
    group_sample_bams_list += group_sample_bams(group)
    group_bam_dict[group] = group_sample_bams(group)


group_sample_coverage_files_list, group_cov_dict = [], {}
def group_sample_coverages_files(target_group, sample_and_group_info = sample_and_group_info):
    target_samples_list = list(sample_and_group_info[sample_and_group_info.group.eq(target_group)]['sample'].values)
    return(expand(config["mapping_dir"] + "{ID}-" + group + "-gene-coverages.tsv", ID = target_samples_list))

for group in group_ID_list:
    group_sample_coverage_files_list += group_sample_coverages_files(group)
    group_cov_dict[group] = group_sample_coverages_files(group)


########################################
############# Rules start ##############
########################################

rule all:
    input:
        config["combined_output_dir"] + "All-combined-KO-function-coverages.tsv",
        config["assemblies_dir"] + "assembly-summaries.tsv",
        config["fastqc_out_dir"] + "raw_multiqc_data.zip",
        config["fastqc_out_dir"] + "filtered_multiqc_data.zip"



rule combine_all_gene_coverages_collapsed_by_KO_and_tax:
    conda:
        "envs/bit.yaml"
    input:
        expand(config["combined_output_dir"] + "{group}-gene-coverages-annotations-and-tax.tsv", group = group_ID_list)
    output:
        config["combined_output_dir"] + "All-combined-KO-function-coverages.tsv",
        config["combined_output_dir"] + "All-combined-KO-function-CPM-normalized-coverages.tsv",
        config["combined_output_dir"] + "All-combined-taxonomy-coverages.tsv",
        config["combined_output_dir"] + "All-combined-taxonomy-CPM-normalized-coverages.tsv"
    params:
        combined_output_dir = config["combined_output_dir"]
    shell:
        """
        python scripts/combine-all-gene-tables.py -o {params.combined_output_dir} {input}
        """

rule combine_group_gene_coverage_annots_and_tax:
    conda:
        "envs/bit.yaml"
    input:
        coverage_files = lambda wildcards: group_cov_dict[wildcards.group],
        ko_tab = config["annotations_and_tax_dir"] + "{group}-annotations.tsv",
        tax_tab = config["annotations_and_tax_dir"] + "{group}-gene-tax.tsv"
    output:
        config["combined_output_dir"] + "{group}-gene-coverages-annotations-and-tax.tsv",
        config["combined_output_dir"] + "{group}-CPM-normalized-gene-coverages-annotations-and-tax.tsv"
    params:
        combined_output_dir = config["combined_output_dir"]
    shell:
        """
        python scripts/combine-gene-level-coverages-annots-and-tax-per-group.py -g {wildcards.group} -a {input.ko_tab} -t {input.tax_tab} -o {params.combined_output_dir} {input.coverage_files}
        """


rule get_cov_and_det:
    """
    This rule pulls out coverage and detection information for each sample, gene-level and transcript-level,
    and filters the coverage information based on requiring at least 50% detection.
    """

    conda:
        "envs/mapping.yaml"
    input:
        bam = config["mapping_dir"] + "{ID}-{group}.bam",
        nt = config["genes_dir"] + "{group}-genes.fa"
    params:
        gene_cov_and_det_tmp = config["mapping_dir"] + "{ID}-{group}-gene-cov-and-det.tmp",
        transcript_cov_and_det_tmp = config["mapping_dir"] + "{ID}-{group}-transcript-cov-and-det.tmp",
        gene_cov_tmp = config["mapping_dir"] + "{ID}-{group}-gene-cov.tmp",
        transcript_cov_tmp = config["mapping_dir"] + "{ID}-{group}-transcript-cov.tmp"
    output:
        gene_covs = config["mapping_dir"] + "{ID}-{group}-gene-coverages.tsv",
        transcript_covs = config["mapping_dir"] + "{ID}-{group}-transcript-coverages.tsv"
    log:
        config["logs_dir"] + "{ID}-{group}-pileup.log"
    shell:
        """
        pileup.sh -in {input.bam} fastaorf={input.nt} outorf={params.gene_cov_and_det_tmp} out={params.transcript_cov_and_det_tmp} > {log} 2>&1

        # filtering coverages based on detection
          # genes
        grep -v "#" {params.gene_cov_and_det_tmp} | awk -F $'\t' ' BEGIN {{OFS=FS}} {{ if ( $10 <= 0.5 ) $4 = 0 }} {{ print $1,$4 }} ' > {params.gene_cov_tmp}
        cat <( printf "gene_ID\tcoverage\n" ) {params.gene_cov_tmp} > {output.gene_covs}

          # transcripts
        grep -v "#" {params.transcript_cov_and_det_tmp} | awk -F $'\t' ' BEGIN {{OFS=FS}} {{ if ( $5 <= 50 ) $2 = 0 }} {{ print $1,$2 }} ' > {params.transcript_cov_tmp}
        cat <( printf "transcript_ID\tcoverage\n" ) {params.transcript_cov_tmp} > {output.transcript_covs}

          # removing intermediate files
        rm {params}
        """


rule run_mapping:
    """
    This rule runs the mapping for each sample to its appropriate group coassembly.
    """
    
    conda:
        "envs/mapping.yaml"
    input:
        mapping_trigger = config["logs_dir"] + "{group}-bowtie2-build.log",
        R1 = config["filtered_reads_dir"] + "{ID}" + config["trimmed_R1_suffix"],
        R2 = config["filtered_reads_dir"] + "{ID}" + config["trimmed_R2_suffix"]
    params:
        index = config["mapping_dir"] + "{group}-index",
        mapping_info = config["logs_dir"] + "{ID}-{group}-mapping-info.txt",
        num_threads = config["num_threads"]
    output:
        config["mapping_dir"] + "{ID}-{group}.bam"
    shell:
        """
        bowtie2 --mm -q --threads {params.num_threads} -x {params.index} -1 {input.R1} -2 {input.R2} --no-unal 2> {params.mapping_info} | samtools view -b | samtools sort -@ {params.num_threads} > {output} 2> /dev/null
        samtools index -@ {params.num_threads} {output}
        """


rule build_bowtie2_index:
    """ Builds bowtie2 databases """

    conda:
        "envs/mapping.yaml"
    input:
        config["assemblies_dir"] + "{group}-coassembly.fa"
    params:
        basename = config["mapping_dir"] + "{group}-index"
    output:
        mapping_trigger = config["logs_dir"] + "{group}-bowtie2-build.log"
    shell:
        """
        bowtie2-build {input} {params.basename} > {output.mapping_trigger} 2>&1
        """


rule run_tax_classification:
    """
    This rule runs the gene- and transcript-level taxonomic classifications for each coassembly.
    """

    conda:
        "envs/cat.yaml"
    input:
        assembly = config["assemblies_dir"] + "{group}-coassembly.fa",
        AA = config["genes_dir"] + "{group}-genes.faa",
        cat_db_trigger = config["REF_DB_ROOT_DIR"] + config["CAT_DIR"] + "/" + config["CAT_TRIGGER_FILE"]
    output:
        gene_tax_out = config["annotations_and_tax_dir"] + "{group}-gene-tax.tsv",
        transcript_tax_out = config["annotations_and_tax_dir"] + "{group}-transcript-tax.tsv"
    params:
        tmp_out_prefix = config["annotations_and_tax_dir"] + "{group}-tax-out.tmp",
        tmp_genes = config["annotations_and_tax_dir"] + "{group}-gene-tax.tmp",
        tmp_transcripts = config["annotations_and_tax_dir"] + "{group}-transcript-tax.tmp",
        cat_db = config["REF_DB_ROOT_DIR"] + config["CAT_DIR"] + config["CAT_DB"],
        cat_tax = config["REF_DB_ROOT_DIR"] + config["CAT_DIR"] + config["CAT_TAX"],
        num_cpus = config["num_cpus"]
    log:
        config["logs_dir"] + "{group}-CAT.log"
    shell:
        """
        CAT contigs -d {params.cat_db} -t {params.cat_tax} -n {params.num_cpus} -r 3 --top 4 --I_know_what_Im_doing -c {input.assembly} -p {input.AA} -o {params.tmp_out_prefix} --force > {log} 2>&1

        # adding names to gene classifications
        CAT add_names -i {params.tmp_out_prefix}.ORF2LCA.txt -o {params.tmp_genes} -t {params.cat_tax} --only_official > {log} 2>&1

        # formatting gene classifications
        awk -F $'\t' ' BEGIN {{ OFS=FS }} {{ if ( $2 == "lineage" ) {{ print $1,$2,$4,$5,$6,$7,$8,$9,$10 }} \
        else if ( $2 == "ORF has no hit to database" || $2 ~ /^no taxid found/ ) {{ print $1,"NA","NA","NA","NA","NA","NA","NA","NA" }} \
        else {{ n=split($2,lineage,";"); print $1,lineage[n],$4,$5,$6,$7,$8,$9,$10 }} }} ' {params.tmp_genes} | \
        sed 's/not classified/NA/g' | sed 's/superkingdom/domain/' | sed 's/^# ORF/gene_ID/' | sed 's/lineage/taxid/' | \
        sed 's/\*//g' > {output.gene_tax_out}

        # adding names to contig classifications
        CAT add_names -i {params.tmp_out_prefix}.contig2classification.txt -o {params.tmp_transcripts} -t {params.cat_tax} --only_official > {log} 2>&1

        # formatting contig classifications
        awk -F $'\t' ' BEGIN {{ OFS=FS }} {{ if ( $2 == "classification" ) {{ print $1,$4,$6,$7,$8,$9,$10,$11,$12 }} \
        else if ( $2 == "unclassified" ) {{ print $1,"NA","NA","NA","NA","NA","NA","NA","NA" }} \
        else {{ n=split($4,lineage,";"); print $1,lineage[n],$6,$7,$8,$9,$10,$11,$12 }} }} ' {params.tmp_transcripts} | \
        sed 's/not classified/NA/g' | sed 's/superkingdom/domain/' | sed 's/: [0-9\.]*//g' | sed 's/^# contig/contig_ID/' | \
        sed 's/lineage/taxid/' | sed 's/\*//g' > {output.transcript_tax_out}

        rm -rf {params.tmp_out_prefix} {params.tmp_genes} {params.tmp_transcripts}
        """


rule run_KO_annotation:
    """
    This rule runs the gene-level (KO) functional annotation for each coassembly.
    """

    conda:
        "envs/kofamscan.yaml"
    input:
        AAs = config["genes_dir"] + "{group}-genes.faa",
        kofamscan_db_trigger = config["REF_DB_ROOT_DIR"] + config["KOFAMSCAN_DIR"] + "/" + config["KOFAMSCAN_TRIGGER_FILE"]
    output:
        config["annotations_and_tax_dir"] + "{group}-annotations.tsv"
    params:
        ko_db_dir = config["REF_DB_ROOT_DIR"] + config["KOFAMSCAN_DIR"],
        tmp_out = config["annotations_and_tax_dir"] + "{group}-KO-tab.tmp",
        tmp_dir = config["annotations_and_tax_dir"] + "{group}-tmp-KO-dir",
        num_cpus = config["num_cpus"]
    log:
        config["logs_dir"] + "{group}-kofamscan.log"
    shell:
        """
        exec_annotation -p {params.ko_db_dir}/profiles/ -k {params.ko_db_dir}/ko_list --cpu {params.num_cpus} -f detail-tsv -o {params.tmp_out} --tmp-dir {params.tmp_dir} --report-unannotated {input.AAs} > {log} 2>&1

        bit-filter-KOFamScan-results -i {params.tmp_out} -o {output}

        rm -rf {params.tmp_out} {params.tmp_dir}
        """


rule call_genes:
    """
    This rule calls genes on each group's coassembly file.
    """

    conda:
        "envs/prodigal.yaml"
    input:
        config["assemblies_dir"] + "{group}-coassembly.fa"
    output:
        AA = config["genes_dir"] + "{group}-genes.faa",
        nt = config["genes_dir"] + "{group}-genes.fa",
        gff = config["genes_dir"] + "{group}-genes.gff"
    log:
        config["logs_dir"] + "{group}-prodigal.log"
    shell:
        """
        prodigal -q -c -p meta -a {output.AA} -d {output.nt} -f gff -o {output.gff} -i {input} > {log} 2>&1
        """


rule summarize_assemblies:
    """
    This rule summarizes and reports general stats for all individual sample assemblies in one table.
    """

    conda:
        "envs/bit.yaml"
    input:
        expand(config["assemblies_dir"] + "{group}-coassembly.fa", group = group_ID_list),
    output:
        config["assemblies_dir"] + "assembly-summaries.tsv"
    shell:
        """
        bit-summarize-assembly -o {output} {input}
        """


rule assembly:
    """
    This rule handles running the co-assembly for each individual group of samples.
    """

    conda:
        "envs/trinity.yaml"
    input:
        R1_reads = lambda wildcards: R1_group_dict[wildcards.group],
        R2_reads = lambda wildcards: R2_group_dict[wildcards.group]
    output:
        config["assemblies_dir"] + "{group}-coassembly.fa"
    params:
        tmp_out_dir = config["assemblies_dir"] + "{group}-trinity-out",
        assemblies_dir = config["assemblies_dir"],
        num_threads = config["num_threads"],
        max_mem = config["max_mem"]
    log:
        config["logs_dir"] + "{group}-trinity.log"
    shell:
        """
        R1_reads=$(echo {input.R1_reads} | tr " " ",")
        R2_reads=$(echo {input.R2_reads} | tr " " ",")

        Trinity --seqType fq --max_memory {params.max_mem} --left ${{R1_reads}} --right ${{R2_reads}} --CPU {params.num_threads} --min_contig_length 500 --output {params.tmp_out_dir} --full_cleanup > {log} 2>&1

        bit-rename-fasta-headers -i {params.tmp_out_dir}.Trinity.fasta -w c_{wildcards.group} -o {output}
        rm {params.tmp_out_dir}.Trinity.fasta.gene_trans_map {params.tmp_out_dir}.Trinity.fasta
        """


rule filtered_fastqc:
    """
    This rule runs fastqc on all trimmed/filtered input fastq files.
    """

    conda:
        "envs/qc.yaml"
    input:
        config["filtered_reads_dir"] + "{ID}" + config["trimmed_R1_suffix"],
        config["filtered_reads_dir"] + "{ID}" + config["trimmed_R2_suffix"]
    output:
        config["filtered_reads_dir"] + "{ID}" + config["trimmed_R1_fastqc_suffix"],
        config["filtered_reads_dir"] + "{ID}" + config["trimmed_R2_fastqc_suffix"]
    shell:
        """
		fastqc {input} -t 2 -q
		"""


rule filtered_multiqc:
    """
    This rule collates all trimmed/filtered fastqc outputs.
    """

    conda:
        "envs/qc.yaml"
    input:
        expand(config["filtered_reads_dir"] + "{ID}" + config["trimmed_R1_fastqc_suffix"], ID = sample_ID_list),
        expand(config["filtered_reads_dir"] + "{ID}" + config["trimmed_R2_fastqc_suffix"], ID = sample_ID_list)
    params:
        fastqc_out_dir = config["fastqc_out_dir"],
        filtered_reads_dir = config["filtered_reads_dir"]
    output:
        config["fastqc_out_dir"] + "filtered_multiqc.html",
        config["fastqc_out_dir"] + "filtered_multiqc_data.zip"
    shell:
        """
        multiqc -z -q -o {params.fastqc_out_dir} -n filtered_multiqc  {params.filtered_reads_dir} > /dev/null 2>&1
          # removing the individual fastqc files and temp locations
        rm -rf {params.filtered_reads_dir}*fastqc*
        """


rule bbduk:
    """
    This rule runs quality filtering/trimming on raw input fastq files for each individual sample.
    """

    conda:
        "envs/qc.yaml"
    input:
        in1 = config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"],
        in2 = config["raw_reads_dir"] + "{ID}" + config["raw_R2_suffix"]
    output:
        out1 = config["filtered_reads_dir"] + "{ID}" + config["trimmed_R1_suffix"],
        out2 = config["filtered_reads_dir"] + "{ID}" + config["trimmed_R2_suffix"]
    log:
        config["logs_dir"] + "bbduk-{ID}.log"
    shell:
        """
        bbduk.sh in={input.in1} in2={input.in2} out1={output.out1} out2={output.out2} \
                ref=${{CONDA_PREFIX}}/opt/bbmap-38.86-0/resources/adapters.fa ktrim=l k=17 ftm=5 qtrim=rl \
                trimq=10 mlf=0.5 maxns=0 > {log} 2>&1
        """


rule raw_multiqc:
    """
    This rule collates all raw fastqc outputs.
    """

    conda:
        "envs/qc.yaml"
    input:
        expand(config["raw_reads_dir"] + "{ID}" + config["raw_R1_fastqc_suffix"], ID = sample_ID_list),
        expand(config["raw_reads_dir"] + "{ID}" + config["raw_R2_fastqc_suffix"], ID = sample_ID_list)
    params:
        raw_reads_dir = config["raw_reads_dir"],
        fastqc_out_dir = config["fastqc_out_dir"]
    output:
        config["fastqc_out_dir"] + "raw_multiqc.html",
        config["fastqc_out_dir"] + "raw_multiqc_data.zip"
    shell:
        """
        multiqc -z -q -o {params.fastqc_out_dir} -n raw_multiqc {params.raw_reads_dir} > /dev/null 2>&1
          # removing the individual fastqc files
        rm -rf {params.raw_reads_dir}*fastqc*
        """


rule raw_fastqc:
    """
    This rule runs fastqc on all raw input fastq files.
    """

    conda:
        "envs/qc.yaml"
    input:
        config["raw_reads_dir"] + "{ID}" + config["raw_R1_suffix"],
        config["raw_reads_dir"] + "{ID}" + config["raw_R2_suffix"]        
    output:
        config["raw_reads_dir"] + "{ID}" + config["raw_R1_fastqc_suffix"],
        config["raw_reads_dir"] + "{ID}" + config["raw_R2_fastqc_suffix"]
    shell:
        """
		fastqc {input} -t 2 -q
		"""


### database checking and setup rules ###
rule setup_CAT_db:
    """
    This rule checks for the CAT reference database, and downloads if needed.
    """

    conda:
        "envs/cat.yaml"
    output:
        cat_db_trigger = config["REF_DB_ROOT_DIR"] + config["CAT_DIR"] + "/" + config["CAT_TRIGGER_FILE"]
    params:
        cat_db_dir = config["REF_DB_ROOT_DIR"] + config["CAT_DIR"],
        compressed_cat = config["REF_DB_ROOT_DIR"] + "CAT_prepare_20210107.tar.gz",
        REF_DB_ROOT_DIR = config["REF_DB_ROOT_DIR"]
    log:
        config["logs_dir"] + "setup-CAT-db.log"
    shell:
        """
        mkdir -p {params.REF_DB_ROOT_DIR}

        curl -L -o {params.compressed_cat} https://tbb.bio.uu.nl/bastiaan/CAT_prepare/CAT_prepare_20210107.tar.gz > {log} 2>&1
        tar -xvzf {params.compressed_cat} -C {params.REF_DB_ROOT_DIR} > {log} 2>&1
        rm {params.compressed_cat}

        touch {output.cat_db_trigger}
        """


rule setup_KoFamScan_db:
    """
    This rule checks for the KoFamScan db (minimally currently) and downloads if needed.
    """

    conda:
        "envs/kofamscan.yaml"
    output:
        kofamscan_db_trigger = config["REF_DB_ROOT_DIR"] + config["KOFAMSCAN_DIR"] + "/" + config["KOFAMSCAN_TRIGGER_FILE"]
    params:
        ko_db_dir = config["REF_DB_ROOT_DIR"] + config["KOFAMSCAN_DIR"],
        compressed_ko_list = config["REF_DB_ROOT_DIR"] + config["KOFAMSCAN_DIR"] + "/ko_list.gz",
        compressed_profiles = config["REF_DB_ROOT_DIR"] + config["KOFAMSCAN_DIR"] + "/profiles.tar.gz"
    log:
        config["logs_dir"] + "setup-kofamscan-db.log"
    shell:
        """
        mkdir -p {params.ko_db_dir}

        curl -L -o {params.compressed_ko_list} ftp://ftp.genome.jp/pub/db/kofam/ko_list.gz > {log} 2>&1
        curl -L -o {params.compressed_profiles} ftp://ftp.genome.jp/pub/db/kofam/profiles.tar.gz > {log} 2>&1
        tar -xzf {params.compressed_profiles} -C {params.ko_db_dir} > {log} 2>&1
        gunzip {params.compressed_ko_list}

        touch {output.kofamscan_db_trigger}
        """


rule setup_checkm_db:
    """
    This rule checks for the checkm db (minimally currently) and downloads if needed and sets location for program.
    """

    conda:
        "envs/checkm.yaml"
    output:
        checkm_db_trigger = config["REF_DB_ROOT_DIR"] + config["CHECKM_DIR"] + "/" + config["CHECKM_TRIGGER_FILE"]
    params:
        checkm_db_dir = config["REF_DB_ROOT_DIR"] + config["CHECKM_DIR"]
    log:
        config["logs_dir"] + "setup-checkm-db.log"
    shell:
        """
        mkdir -p {params.checkm_db_dir}
        cd {params.checkm_db_dir}

        curl -LO https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gz > {log} 2>&1
        tar -xzvf checkm_data_2015_01_16.tar.gz > {log} 2>&1

        checkm data setRoot $PWD > {log} 2>&1

        cd - > /dev/null 2>&1

        touch {output.checkm_db_trigger}
        """


rule setup_gtdbtk_db:
    """
    This rule checks for the gtdb-tk db (minimally currently) and downloads if needed.
    """

    conda:
        "envs/gtdb-tk.yaml"
    output:
        gtdbtk_db_trigger = config["REF_DB_ROOT_DIR"] + config["GTDB_DATA_PATH"] + "/" + config["GTDB_TRIGGER_FILE"]
    params:
        gtdbtk_db_dir = config["REF_DB_ROOT_DIR"] + config["GTDB_DATA_PATH"]
    log:
        config["logs_dir"] + "setup-gtdbtk-db.log"
    shell:
        """
        mkdir -p {params.gtdbtk_db_dir}
        cd {params.gtdbtk_db_dir} 

        # adding wanted location to this conda env PATH (gtdb-tk looks in the GTDBTK_DATA_PATH variable),
            # so will be set when the conda environment is started from now on
        mkdir -p ${{CONDA_PREFIX}}/etc/conda/activate.d/
        echo 'export GTDBTK_DATA_PATH={params.gtdbtk_db_dir}' >> ${{CONDA_PREFIX}}/etc/conda/activate.d/set_env_vars.sh

        # but still needs to be set for this particular session that is downloading and setting up the db
        GTDBTK_DATA_PATH={params.gtdbtk_db_dir}

        # now downloading
        download-db.sh > {log} 2>&1

        cd - > /dev/null

        touch {output.gtdbtk_db_trigger}
        """


rule clean_all:
    shell:
        "rm -rf {dirs_to_create} .snakemake/ snakemake-run.log"
